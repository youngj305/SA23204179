---
title: "Introduction to SA23204179 Homework"
author: "Jian Yang"
date: "2023-11-27"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SA23204179 Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

SA23204179 is a R package, including 2023 fall statistical computing course homework, the following instructions are thest functions.


## Homework1


In homework1, we learned how to generate random number, and it includes “Use inverse transformation method to reproduce the function Sample when replace=TRUE”, “Use the inverse transform method to generate a random sample of size n from the standard Laplace distribution(Exercise 3.2)”, “Use the acceptance-rejection method to generate a random sample of size n from the Beta(a,b)(Exercise 3.7)” and “Generate a random sample of size n for which the the pdf is rescaled Epanechnikov kernel(Exercise 3.9)”

```{r}
library(SA23204179)
n<-1000
X1<-my.sample(1:5,n)
X2<-my.sample(1:5,n,prob = c(1,2,3,4,5))
X3<-my.sample(c(3,2,1,4,5),n,prob = c(5,4,3,2,1))
X4<-my.laplace(1000)
X5<-my.rbeta(1000,3,2)
X6<-rEpa(1000)
```

## Homework2

In homework2, we learned how to compute the Monte Carlo integration, and it includes “Compute the percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variate method compared with simple MC (Exercise 5.6)and”Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method(Exercise 5.7)”

```{r}
my_pi(0.5)
my_pi(0.8)
my_pi(1)
result<-mc(1000)
mean(result$simple)
mean(result$anti)
(var(result$simple)-var(result$anti))/var(result$simple)
```




## Homework3

In homework3, we learned Monte Carlo Methods in Inference, and it includes “Compute a Monte Carlo estimate by importance sampling(Exercise 5.14)”, “Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples(Exercise 6.5)” and “Use Monte Carlo simulation to investigate the empirical Type I error rate(Exercise 6.A)”

```{r}
a<-mc1(1e4)
b<-mc2(2000)
c<-mc3(20,1e4)
```



## Homework4

In homewrok4, we learned bootstrap, and it includes “Compare the mean bias and the sd of bootstrap estimate with the theoretical ones” and “Obtain a bootstrap t confidence interval estimate for the correlation statistic(Exercise 7.3)”


```{r}
a1<-B_B.H(1000)
set.seed(1234)
n5<-my.boots(5)
n10<-my.boots(10)
n20<-my.boots(20)
round(rbind(n5,n10,n20),4)
```



## Homework5

In homework5, we learned jackknife estimate, cross validation, and it includes “Compute 95% bootstrap confidence intervals by the standard normal, basic, percentile, and BCa methods(Exercise 7.5)”, “Obtain jackknife estimates of bias and standard error(Exercise 7.8)”, and “Use leave-two-out cross validation to compare the models(Exercise 7.11)”

```{r}
library(bootstrap)
jackn(scor)
```




## Homework6

In homework6, we learned permutation, and it includes “Implement the two-sample Cramer-von Mises test for equal distributions as a permutation test(Exercise 8.1)”and “Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal(Exercise 8.3)”

```{r}
library(boot)
set.seed(1234)
attach(chickwts)
x <- as.vector(weight[feed == "soybean"]);y <- as.vector(weight[feed == "linseed"])
detach(chickwts)
z <- c(x, y)
N <- c(length(x), length(y))
boot.obj <- boot(data = z, statistic = W2, R = 9999,sim = "permutation", sizes = N)
ts <- c(boot.obj$t0,boot.obj$t)
mean(ts>=ts[1])
```


## Homework7

In homework7, we learned Markov Chain Monte Carlo Methods, and it includes “Implement a random walk Metropolis sampler for generating the standard Laplace distribution(Exercise 9.4)”and “Implement a Gibbs sampler to generate a bivariate normal chain with zero means, unit standard deviations, and correlation 0.9.(Exercise 9.7)”

```{r}
solve_alpha(1e6,0,1,-1,0.1)
a7<-rw.Metropolis(.5, 100, 10000)
b7<-my.gibbs(15000,5000)
```


## Homework8

In homework8, we learned Numerical Methods in R, and it includes “Implement a numeric solve of the EM algorithm”and “Compare it to MLE”

```{r}
u<-c(11,8,27,13,16,0,23,10,24,2)
v<-c(12,9,28,14,17,1,24,11,25,3)
em(1,u,v)
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
result <- solve_game(A)
```

## Homework9

In homework9, we learned High Performance Computation with Rcpp, and it includes “Compare the computation time of the Gibbs sampler with R and Gibbs sampler with Rcpp by microbenchmark.”

```{r}
library(Rcpp)
library(microbenchmark)
a<-2
b<-3
n<-10
num<-1000
ts <- microbenchmark(gibbR=r_gibbs(num,a,b,n),gibbC=rcpp_gibbs(num,a,b,n))
ts
```


